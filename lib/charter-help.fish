set translations translated/mvpd/charter

# Column name constants
set -l asd_col "Audience Segment Name"
set -l date_col "Event Date"
set -l time_col "Event Time"
set -l network_col "Network"
set -l isci_col "ISCI/ADID"
set -l impressions_col "Impressions Delivered"

function charter-spot-files
  ls $translations/*.xlsx.csv
  ls $translations/*_details.txt.csv
  ls $translations/*_details_*.txt.csv
end

function sorted-charter-spot-files
  charter-spot-files | bin/sort-translations-by-upload-time
end

function charter-megasheet
  sorted-charter-spot-files | xargs xsv cat rows
end

function filter-segments
  set -l pattern $argv[1]

  xsv search -s 'Audience Segment Name' $pattern
end

function select-relevant-fields
  xsv select 'Audience Segment Name','Event Date','Event Time','Network','ISCI/ADID','Impressions Delivered'
end

function sum-nums
  ruby -e 'puts ARGF.each_line.map(&:to_i).reduce(&:+)'
end

function limit-dates
  xsv search -s 'Event Date' (bin/date-range-regex $argv)
end

function just-impressions
  xsv select 'Impressions Delivered' | tail -n +2
end

function impressions-napkincalc \
  --argument order_number start_date end_date data_file \
  --description "Calculates the number of impressions for `order_number`\
  between `start_date` and `end_date` (inclusive), optionally using `data_file`\
  instead of generating the megasheet each time."

  if test -n "$data_file"
    cat "$data_file"
  else
    charter-megasheet
end | filter-segments "$order_number" | limit-dates "$start_date" "$end_date" | select-relevant-fields | bin/accumulate | just-impressions | sum-nums
end

function check-charter-mismatch-file --argument file \
  --description "For each line of a mismatch CSV file, as generated by our bot,\
  attaches our napkin-math impressions number to compare to Redshift and BQ.\
  Outputs CSV."

  set -l cache "deduped-charter-megasheet.csv" #(mktemp)
  # echo "Caching megasheet in $cache..." >&2
  # charter-megasheet > "$cache"

  function check-line
    set -l raw_line (string trim "$argv[1]")
    set -l line (string split "," "$raw_line")
    set -l order "$line[1]"
    set -l earliest "$line[4]"
    set -l latest "$line[5]"

    set -l napkin_imps (impressions-napkincalc "$order" "$earliest" "$latest" "$cache")
    if test -z "$napkin_imps"
      set napkin_imps 0
    end

    echo "$raw_line,$napkin_imps"
  end

  set -l awaiting_headers true

  for line in (cat $file)
    set -l trimmed_line (string trim "$line")

    if $awaiting_headers
      set awaiting_headers false
      echo "$trimmed_line,napkin_imps"
    else
      check-line "$trimmed_line"
    end
  end
end

function get-vantage-spots \
  --argument data_file \
  --description "Selects all rows with _VA_ in the Audience Segment Name."

  xsv search -s $asd_col '_VA_' $data_file
end

function get-unique-spots \
  --argument data_file \
  --description "Filters rows within data file to those with unique spots (where spot = date + time + network + isci)."

  xsv select $date_col,$time_col,$network_col,$isci_col | sort -u
end


function contains-underscore-num-asd \
  --argument data_file \
  --description "Looks for any ASD values ending in \"_1\", \"_2\", etc."

  set underscore_num_asd (xsv search -s $asd_col '^.*_[0-9]$' $data_file \
      | xsv select $asd_col \
      | tail -n +2 \
      | sort -u)
  if test (count $underscore_num_asd) -gt 0
      echo "Contains the following Audience Segment Names ending in _1, _2, etc.:"
      echo $underscore_num_asd
      return 1
  end
end

function contains-duplicate-spots \
  --argument data_file \
  --description "Calculates the number of duplicate spots in the given file."

  set num_spots (xsv count $data_file)
  set num_unique_spots (math (xsv select $date_col,$time_col,$network_col,$isci_col $data_file\
      | sort -u \
      | wc -l) - 1) # subtract 1 from the line count because one of those lines is the header
  set num_dupes (math $num_spots - $num_unique_spots)
  echo $num_dupes
  if test $num_dupes -gt 0
      echo "Contains $num_dupes duplicate spots."\n
      return 1
  end
end
